{"m1": 4.079663276672363, "m2": 1.1422924995422363, "reason": "Spark sorting is more efficient than python sorting in this case. Possible reason can be that Spark utilized the distributed structure by sorting the partitions in parallel in different clusters then merge together. The distributed computation can significantly save computation time as the operations are done on different partitions of the datasets at the same time. Python perform sorting of the entire dataset in memory, without parallel computation, it is very likely to be slower than Spark sorting"}